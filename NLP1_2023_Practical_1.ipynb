{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1-aRiOgl4nHg"
      },
      "source": [
        "------\n",
        "**You cannot save any changes you make to this file, so please make sure to save it on your Google Colab drive or download it as a .ipynb file.**\n",
        "\n",
        "------\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lIZrAUx57vsM"
      },
      "source": [
        "Practical 1: Sentiment Detection in Movie Reviews\n",
        "========================================\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J4kXPMhyngZW"
      },
      "source": [
        "This practical concerns detecting sentiment in movie reviews. This is a typical NLP classification task.\n",
        "In [this file](https://gist.githubusercontent.com/bastings/d47423301cca214e3930061a5a75e177/raw/5113687382919e22b1f09ce71a8fecd1687a5760/reviews.json) (80MB) you will find 1000 positive and 1000 negative **movie reviews**.\n",
        "Each review is a **document** and consists of one or more sentences.\n",
        "\n",
        "To prepare yourself for this practical, you should\n",
        "have a look at a few of these texts to understand the difficulties of\n",
        "the task: how might one go about classifying the texts? You will write\n",
        "code that decides whether a movie review conveys positive or\n",
        "negative sentiment.\n",
        "\n",
        "Please make sure you have read the following paper:\n",
        "\n",
        ">   Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan\n",
        "(2002).\n",
        "[Thumbs up? Sentiment Classification using Machine Learning\n",
        "Techniques](https://dl.acm.org/citation.cfm?id=1118704). EMNLP.\n",
        "\n",
        "Bo Pang et al. introduced the movie review sentiment\n",
        "classification task, and the above paper was one of the first papers on\n",
        "the topic. The first version of your sentiment classifier will do\n",
        "something similar to Pang et al.'s system. If you have questions about it,\n",
        "you should resolve you doubts as soon as possible with your TA.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cb7errgRASzZ"
      },
      "source": [
        "**Advice**\n",
        "\n",
        "Please read through the entire practical and familiarise\n",
        "yourself with all requirements before you start coding or otherwise\n",
        "solving the tasks. Writing clean and concise code can make the difference\n",
        "between solving the assignment in a matter of hours, and taking days to\n",
        "run all experiments.\n",
        "\n",
        "\n",
        "**Implementation**\n",
        "\n",
        "While we inserted code cells to indicate where you should implement your own code, please feel free to add/remove code blocks where you see fit (but make sure that the general structure of the assignment is preserved). Also, please keep in mind that it is always good practice to structure your code properly, e.g., by implementing separate classes and functions that can be reused.\n",
        "\n",
        "## Environment\n",
        "\n",
        "All code should be written in **Python 3**.\n",
        "This is the default in Google Colab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SaZnxptMJiD7",
        "outputId": "930aaa36-6b58-4e8d-bc98-afef70684e5c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Python 3.10.12\n"
          ]
        }
      ],
      "source": [
        "!python --version"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BYZyIF7lJnGn"
      },
      "source": [
        "If you want to run code on your own computer, then download this notebook through `File -> Download .ipynb`.\n",
        "The easiest way to\n",
        "install Python is through downloading\n",
        "[Anaconda](https://www.anaconda.com/download).\n",
        "After installation, you can start the notebook by typing `jupyter notebook filename.ipynb`.\n",
        "You can also use an IDE\n",
        "such as [PyCharm](https://www.jetbrains.com/pycharm/download/) to make\n",
        "coding and debugging easier. It is good practice to create a [virtual\n",
        "environment](https://docs.python.org/3/tutorial/venv.html) for this\n",
        "project, so that any Python packages don’t interfere with other\n",
        "projects.\n",
        "\n",
        "\n",
        "**Learning Python 3**\n",
        "\n",
        "If you are new to Python 3, you may want to check out a few of these resources:\n",
        "- https://learnxinyminutes.com/docs/python3/\n",
        "- https://www.learnpython.org/\n",
        "- https://docs.python.org/3/tutorial/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hok-BFu9lGoK"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import os\n",
        "import sys\n",
        "from subprocess import call\n",
        "from nltk import FreqDist\n",
        "from nltk.util import ngrams\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "import sklearn as sk\n",
        "from google.colab import drive\n",
        "import pickle\n",
        "import json\n",
        "from collections import Counter\n",
        "import requests\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bXWyGHwE-ieQ"
      },
      "source": [
        "## Loading the data\n",
        "\n",
        "**Download the sentiment lexicon and the movie reviews dataset.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lm-rakqtlMOT",
        "outputId": "c95d7eaa-6160-4e66-b17f-7472e321774d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2023-11-15 16:07:44--  https://gist.githubusercontent.com/bastings/d6f99dcb6c82231b94b013031356ba05/raw/f80a0281eba8621b122012c89c8b5e2200b39fd6/sent_lexicon\n",
            "Resolving gist.githubusercontent.com (gist.githubusercontent.com)... 185.199.110.133, 185.199.109.133, 185.199.111.133, ...\n",
            "Connecting to gist.githubusercontent.com (gist.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 662577 (647K) [text/plain]\n",
            "Saving to: ‘sent_lexicon’\n",
            "\n",
            "\rsent_lexicon          0%[                    ]       0  --.-KB/s               \rsent_lexicon        100%[===================>] 647.05K  --.-KB/s    in 0.03s   \n",
            "\n",
            "2023-11-15 16:07:45 (23.5 MB/s) - ‘sent_lexicon’ saved [662577/662577]\n",
            "\n",
            "--2023-11-15 16:07:45--  https://gist.githubusercontent.com/bastings/d47423301cca214e3930061a5a75e177/raw/5113687382919e22b1f09ce71a8fecd1687a5760/reviews.json\n",
            "Resolving gist.githubusercontent.com (gist.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to gist.githubusercontent.com (gist.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 83503869 (80M) [text/plain]\n",
            "Saving to: ‘reviews.json’\n",
            "\n",
            "reviews.json        100%[===================>]  79.63M   188MB/s    in 0.4s    \n",
            "\n",
            "2023-11-15 16:07:45 (188 MB/s) - ‘reviews.json’ saved [83503869/83503869]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# download sentiment lexicon\n",
        "!wget https://gist.githubusercontent.com/bastings/d6f99dcb6c82231b94b013031356ba05/raw/f80a0281eba8621b122012c89c8b5e2200b39fd6/sent_lexicon\n",
        "# download review data\n",
        "!wget https://gist.githubusercontent.com/bastings/d47423301cca214e3930061a5a75e177/raw/5113687382919e22b1f09ce71a8fecd1687a5760/reviews.json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AkPwuHp5LSuQ"
      },
      "source": [
        "**Load the movie reviews.**\n",
        "\n",
        "Each word in a review comes with its part-of-speech tag. For documentation on POS-tags, see https://catalog.ldc.upenn.edu/docs/LDC99T42/tagguid1.pdf.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "careEKj-mRpl",
        "outputId": "00458d93-56d6-4d4d-dda3-57471923b335"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total number of reviews: 2000 \n",
            "\n",
            "0 NEG 29\n",
            "Two/CD teen/JJ couples/NNS go/VBP to/TO a/DT church/NN party/NN ,/, drink/NN and/CC then/RB drive/NN ./.\n",
            "1 NEG 11\n",
            "Damn/JJ that/IN Y2K/CD bug/NN ./.\n",
            "2 NEG 24\n",
            "It/PRP is/VBZ movies/NNS like/IN these/DT that/WDT make/VBP a/DT jaded/JJ movie/NN viewer/NN thankful/JJ for/IN the/DT invention/NN of/IN the/DT Timex/NNP IndiGlo/NNP watch/NN ./.\n",
            "3 NEG 19\n",
            "QUEST/NN FOR/IN CAMELOT/NNP ``/`` Quest/NNP for/IN Camelot/NNP ''/'' is/VBZ Warner/NNP Bros./NNP '/POS first/JJ feature-length/JJ ,/, fully-animated/JJ attempt/NN to/TO steal/VB clout/NN from/IN Disney/NNP 's/POS cartoon/NN empire/NN ,/, but/CC the/DT mouse/NN has/VBZ no/DT reason/NN to/TO be/VB worried/VBN ./.\n",
            "4 NEG 38\n",
            "Synopsis/NNPS :/: A/DT mentally/RB unstable/JJ man/NN undergoing/VBG psychotherapy/NN saves/VBZ a/DT boy/NN from/IN a/DT potentially/RB fatal/JJ accident/NN and/CC then/RB falls/VBZ in/IN love/NN with/IN the/DT boy/NN 's/POS mother/NN ,/, a/DT fledgling/NN restauranteur/NN ./.\n",
            "\n",
            "Number of word types: 47743\n",
            "Number of word tokens: 1512359\n",
            "\n",
            "Most common tokens:\n",
            "         , :    77842\n",
            "       the :    75948\n",
            "         . :    59027\n",
            "         a :    37583\n",
            "       and :    35235\n",
            "        of :    33864\n",
            "        to :    31601\n",
            "        is :    25972\n",
            "        in :    21563\n",
            "        's :    18043\n",
            "        it :    15904\n",
            "      that :    15820\n",
            "     -rrb- :    11768\n",
            "     -lrb- :    11670\n",
            "        as :    11312\n",
            "      with :    10739\n",
            "       for :     9816\n",
            "       his :     9542\n",
            "      this :     9497\n",
            "      film :     9404\n"
          ]
        }
      ],
      "source": [
        "# file structure:\n",
        "# [\n",
        "#  {\"cv\": integer, \"sentiment\": str, \"content\": list}\n",
        "#  {\"cv\": integer, \"sentiment\": str, \"content\": list}\n",
        "#   ..\n",
        "# ]\n",
        "# where `content` is a list of sentences,\n",
        "# with a sentence being a list of (token, pos_tag) pairs.\n",
        "\n",
        "\n",
        "with open(\"reviews.json\", mode=\"r\", encoding=\"utf-8\") as f:\n",
        "  reviews = json.load(f)\n",
        "\n",
        "print(\"Total number of reviews:\", len(reviews), '\\n')\n",
        "\n",
        "def print_sentence_with_pos(s):\n",
        "  print(\" \".join(\"%s/%s\" % (token, pos_tag) for token, pos_tag in s))\n",
        "\n",
        "for i, r in enumerate(reviews):\n",
        "  print(r[\"cv\"], r[\"sentiment\"], len(r[\"content\"]))  # cv, sentiment, num sents\n",
        "  print_sentence_with_pos(r[\"content\"][0])\n",
        "  if i == 4:\n",
        "    break\n",
        "\n",
        "c = Counter()\n",
        "for review in reviews:\n",
        "  for sentence in review[\"content\"]:\n",
        "    for token, pos_tag in sentence:\n",
        "      c[token.lower()] += 1\n",
        "\n",
        "print(\"\\nNumber of word types:\", len(c))\n",
        "print(\"Number of word tokens:\", sum(c.values()))\n",
        "\n",
        "print(\"\\nMost common tokens:\")\n",
        "for token, count in c.most_common(20):\n",
        "  print(\"%10s : %8d\" % (token, count))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E6PWaEoh8B34"
      },
      "source": [
        "#(1) Lexicon-based approach (3.5pts)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JsTSMb6ma4E8"
      },
      "source": [
        "A traditional approach to classify documents according to their sentiment is the lexicon-based approach. To implement this approach, you need a **sentiment lexicon**, i.e., a list of words annotated with a sentiment label (e.g., positive and negative, or a score from 0 to 5).\n",
        "\n",
        "In this practical, you will use the sentiment\n",
        "lexicon released by Wilson et al. (2005).\n",
        "\n",
        "> Theresa Wilson, Janyce Wiebe, and Paul Hoffmann\n",
        "(2005). [Recognizing Contextual Polarity in Phrase-Level Sentiment\n",
        "Analysis](http://www.aclweb.org/anthology/H/H05/H05-1044.pdf). HLT-EMNLP.\n",
        "\n",
        "Pay attention to all the information available in the sentiment lexicon. The field *word1* contains the lemma, *priorpolarity* contains the sentiment label (positive, negative, both, or neutral), *type* gives you the magnitude of the word's sentiment (strong or weak), and *pos1* gives you the part-of-speech tag of the lemma. Some lemmas can have multiple part-of-speech tags and thus multiple entries in the lexicon. The path of the lexicon file is `\"sent_lexicon\"`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ogq0Eq2hQglh",
        "outputId": "cedb6480-07b3-4c1f-f059-25fe8ccdcc82"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "type=weaksubj len=1 word1=abandoned pos1=adj stemmed1=n priorpolarity=negative\n",
            "type=weaksubj len=1 word1=abandonment pos1=noun stemmed1=n priorpolarity=negative\n",
            "type=weaksubj len=1 word1=abandon pos1=verb stemmed1=y priorpolarity=negative\n",
            "type=strongsubj len=1 word1=abase pos1=verb stemmed1=y priorpolarity=negative\n",
            "type=strongsubj len=1 word1=abasement pos1=anypos stemmed1=y priorpolarity=negative\n"
          ]
        }
      ],
      "source": [
        "with open(\"sent_lexicon\", mode=\"r\", encoding=\"utf-8\") as f:\n",
        "  line_cnt = 0\n",
        "  for line in f:\n",
        "    print(line.strip())\n",
        "    line_cnt += 1\n",
        "    if line_cnt > 4:\n",
        "      break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mml4nOtIUBhn"
      },
      "source": [
        "Lexica such as this can be used to solve\n",
        "the classification task without using Machine Learning. For example, one might look up every word $w_1 ... w_n$ in a document, and compute a **binary score**\n",
        "$S_{binary}$ by counting how many words have a positive or a\n",
        "negative label in the sentiment lexicon $SLex$.\n",
        "\n",
        "$$S_{binary}(w_1 w_2 ... w_n) = \\sum_{i = 1}^{n}\\text{sign}(SLex\\big[w_i\\big])$$\n",
        "\n",
        "where $\\text{sign}(SLex\\big[w_i\\big])$ refers to the polarity of $w_i$.\n",
        "\n",
        "**Threshold.** On average, there are more positive than negative words per review (~7.13 more positive than negative per review) to take this bias into account you should use a threshold of **8** (roughly the bias itself) to make it harder to classify as positive.\n",
        "\n",
        "$$\n",
        "\\text{classify}(S_{binary}(w_1 w_2 ... w_n)) = \\bigg\\{\\begin{array}{ll}\n",
        "        \\text{positive} & \\text{if } S_{binary}(w_1w_2...w_n) > threshold\\\\\n",
        "        \\text{negative} & \\text{otherwise}\n",
        "        \\end{array}\n",
        "$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tOFnMvbeeZrc"
      },
      "source": [
        "#### (Q1.1) Implement this approach and report its classification accuracy. (1 pt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ED2aTEYutW1-"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE\n",
        "\n",
        "# loading sentiment lexicon of each word\n",
        "def load_sentiment_lexicon():\n",
        "  lexicon = {}\n",
        "  with open(\"sent_lexicon\", mode=\"r\", encoding=\"utf-8\") as f:\n",
        "    for line in f:\n",
        "      entries = line.strip().split()\n",
        "      word = entries[2].split('=')[1]\n",
        "      ppolarity = entries[5].split('=')[1]\n",
        "\n",
        "      if ppolarity == 'positive':\n",
        "        lexicon[word] = 1\n",
        "      elif ppolarity == 'negative':\n",
        "        lexicon[word] = -1\n",
        "\n",
        "    return lexicon\n",
        "\n",
        "\n",
        "# calculating binary_score of review\n",
        "def calculate_sentiment_score(review, lex, threshold):\n",
        "  bscore_rev = 0\n",
        "  for sentence in review[\"content\"]:\n",
        "    for token, pos_tag in sentence:\n",
        "      word = token.lower()\n",
        "      bscore_rev += lex.get(word, 0) # return 0 instead of None if word is not in lexicon\n",
        "\n",
        "  return 1 if bscore_rev > threshold else 0\n",
        "\n",
        "sent_lexicon = load_sentiment_lexicon()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iy528EUTphz5",
        "outputId": "63c018d6-2c47-4d7a-bcaa-388ea134dfa7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.68\n"
          ]
        }
      ],
      "source": [
        "# token_results should be a list of binary indicators; for example [1, 0, 1, ...]\n",
        "# where 1 indicates a correct classification and 0 an incorrect classification\n",
        "token_results = []\n",
        "binary_threshold = 8\n",
        "for review in reviews:\n",
        "  predicted_senti = calculate_sentiment_score(review, sent_lexicon, binary_threshold)\n",
        "  actual_senti = 1 if review['sentiment'] == 'POS' else 0\n",
        "\n",
        "  # compare predicted and true value\n",
        "  if predicted_senti == actual_senti:\n",
        "    token_results.append(1)\n",
        "  else:\n",
        "    token_results.append(0)\n",
        "\n",
        "token_accuracy = sum(token_results) / len(token_results)\n",
        "print(\"Accuracy: %0.2f\" % token_accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Twox0s_3eS0V"
      },
      "source": [
        "As the sentiment lexicon also has information about the **magnitude** of\n",
        "sentiment (e.g., *“excellent\"* has the same sentiment _polarity_ as *“good\"* but it has a higher magnitude), we can take a more fine-grained approach by adding up all\n",
        "sentiment scores, and deciding the polarity of the movie review using\n",
        "the sign of the weighted score $S_{weighted}$.\n",
        "\n",
        "$$S_{weighted}(w_1w_2...w_n) = \\sum_{i = 1}^{n}SLex\\big[w_i\\big]$$\n",
        "\n",
        "\n",
        "Make sure you define an appropriate threshold for this approach.\n",
        "\n",
        "#### (Q1.2) Now incorporate magnitude information and report the classification accuracy. Don't forget to use the threshold. (1pt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qG3hUDnPtkhS"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE\n",
        "\n",
        "# same as before but updated with magnitude\n",
        "def load_sentiment_lexicon(use_magnitude=False):\n",
        "  lexicon = {}\n",
        "  with open(\"sent_lexicon\", mode=\"r\", encoding=\"utf-8\") as f:\n",
        "    for line in f:\n",
        "      entries = line.strip().split()\n",
        "      word = entries[2].split('=')[1]\n",
        "      ppolarity = entries[5].split('=')[1]\n",
        "      if use_magnitude:\n",
        "        type_strength = entries[0].split('=')[1]  # get magnitude\n",
        "        # determining weight based on the magnitude\n",
        "        if type_strength == 'strongsubj':\n",
        "          weight = 5\n",
        "        elif type_strength == 'weaksubj':\n",
        "          weight = 1\n",
        "      else:\n",
        "          weight = 1\n",
        "\n",
        "      if ppolarity == 'positive':\n",
        "        lexicon[word] = weight\n",
        "      elif ppolarity == 'negative':\n",
        "        lexicon[word] = -1 * weight\n",
        "\n",
        "    return lexicon\n",
        "\n",
        "sent_lexicon = load_sentiment_lexicon(use_magnitude=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9vVk7CvDpyka",
        "outputId": "2144cb2a-ea4e-45bb-9f2a-5f43e2c39fd5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.70\n"
          ]
        }
      ],
      "source": [
        "magnitude_results = [] # a list of binary indicators\n",
        "magnitude_threshold = 18\n",
        "for review in reviews:\n",
        "  predicted_senti = calculate_sentiment_score(review, sent_lexicon, magnitude_threshold)\n",
        "  actual_senti = 1 if review['sentiment'] == 'POS' else 0\n",
        "\n",
        "  # comparing predicted and true value\n",
        "  if predicted_senti == actual_senti:\n",
        "    magnitude_results.append(1)\n",
        "  else:\n",
        "    magnitude_results.append(0)\n",
        "\n",
        "magnitude_accuracy = sum(magnitude_results) / len(magnitude_results)\n",
        "print(\"Accuracy: %0.2f\" % magnitude_accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h9SHoGPfsAHV"
      },
      "source": [
        "#### (Q.1.3) Make a barplot of the two results (0.5pt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 452
        },
        "id": "8LgBcYcXsEk3",
        "outputId": "0037b01d-0a47-4d6c-d916-a21e34a770b6"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# YOUR CODE HERE\n",
        "\n",
        "plt.bar([\"binary\", \"magnitude\"], [token_accuracy, magnitude_accuracy])\n",
        "plt.title(\"Classification Accuracy (Lexicon-based Approach)\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sNhS8OCVxMHd"
      },
      "source": [
        "#### (Q1.4) A better threshold (1pt)\n",
        "Above we have defined a threshold to account for an inherent bias in the dataset: there are more positive than negative words per review.\n",
        "However, that threshold does not take into account *document length*. Explain why this is a problem and implement an alternative way to compute the threshold."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xo7gk1I-omLI"
      },
      "source": [
        "Using a fixed threshold doesn't take into account the document length. This can be problematic because a very long review would have more opportunities to include positive words not necessarily because it's expressing a stronger positive sentiment but because of its length. So if this long review is just long enough to pass the threshold, it would be classified as positive even if it might not be. This can result in longer reviews being classified as positive just because of their length, and not because of their sentiment. For example, a long review with 8 positive words out of 100 could be predicted as positive, even if the remaining words are negative (or neutral). Hence, instead of fixed threshold we could normalize the sentiment score by the length of the document. In other words, we can calculate the average sentiment per word. This will make the threshold adaptive to the length of the document."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dwt0B8h8aKjr",
        "outputId": "a0c2c22f-a5d3-46a3-a579-b963b47c5fde"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.70\n"
          ]
        }
      ],
      "source": [
        "# YOUR CODE HERE\n",
        "\n",
        "# calculating normalized binary_score of review\n",
        "def calculate_sentiment_score(review, lex, threshold):\n",
        "  bscore_rev = 0\n",
        "  total_tokens = 0\n",
        "  for sentence in review[\"content\"]:\n",
        "    for token, pos_tag in sentence:\n",
        "      word = token.lower()\n",
        "      bscore_rev += lex.get(word, 0)\n",
        "      total_tokens += 1\n",
        "\n",
        "  # calculating average score per word\n",
        "  norm_score = bscore_rev / total_tokens\n",
        "  return 1 if norm_score > threshold else 0\n",
        "\n",
        "\n",
        "results = [] # a list of binary indicators\n",
        "threshold = 0.04\n",
        "for review in reviews:\n",
        "  predicted_senti = calculate_sentiment_score(review, sent_lexicon, threshold)\n",
        "  actual_senti = 1 if review['sentiment'] == 'POS' else 0\n",
        "  # comparing predicted and true value\n",
        "  if predicted_senti == actual_senti:\n",
        "    results.append(1)\n",
        "  else:\n",
        "    results.append(0)\n",
        "\n",
        "accuracy = sum(results) / len(results)\n",
        "print(\"Accuracy: %0.2f\" % accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LibV4nR89BXb"
      },
      "source": [
        "# (2) Naive Bayes (9.5pts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fnF9adQnuwia"
      },
      "source": [
        "\n",
        "Your second task is to program a simple Machine Learning approach that operates\n",
        "on a simple Bag-of-Words (BoW) representation of the text data, as\n",
        "described by Pang et al. (2002). In this approach, the only features we\n",
        "will consider are the words in the text themselves, without bringing in\n",
        "external sources of information. The BoW model is a popular way of\n",
        "representing texts as vectors, making it\n",
        "easy to apply classical Machine Learning algorithms on NLP tasks.\n",
        "However, the BoW representation is also very crude, since it discards\n",
        "all information related to word order and grammatical structure in the\n",
        "original text—as the name suggests.\n",
        "\n",
        "## Writing your own classifier (4pts)\n",
        "\n",
        "Write your own code to implement the Naive Bayes (NB) classifier. As\n",
        "a reminder, the Naive Bayes classifier works according to the following\n",
        "equation:\n",
        "$$\\hat{c} = \\operatorname*{arg\\,max}_{c \\in C} P(c|\\bar{f}) = \\operatorname*{arg\\,max}_{c \\in C} P(c)\\prod^n_{i=1} P(f_i|c)$$\n",
        "where $C = \\{ \\text{POS}, \\text{NEG} \\}$ is the set of possible classes,\n",
        "$\\hat{c} \\in C$ is the most probable class, and $\\bar{f}$ is the feature\n",
        "vector. Remember that we use the log of these probabilities when making\n",
        "a prediction:\n",
        "$$\\hat{c} = \\operatorname*{arg\\,max}_{c \\in C} \\Big\\{\\log P(c) + \\sum^n_{i=1} \\log P(f_i|c)\\Big\\}$$\n",
        "\n",
        "You can find more details about Naive Bayes in [Jurafsky &\n",
        "Martin](https://web.stanford.edu/~jurafsky/slp3/). You can also look at\n",
        "this helpful\n",
        "[pseudo-code](https://nlp.stanford.edu/IR-book/html/htmledition/naive-bayes-text-classification-1.html).\n",
        "\n",
        "*Note: this section and the next aim to put you in a position to replicate\n",
        "    Pang et al.'s Naive Bayes results. However, your numerical results\n",
        "    will differ from theirs, as they used different data.*\n",
        "\n",
        "**You must write the Naive Bayes training and prediction code from\n",
        "scratch.** You will not be given credit for using off-the-shelf Machine\n",
        "Learning libraries.\n",
        "\n",
        "The data contains the text of the reviews, where each document consists\n",
        "of the sentences in the review, the sentiment of the review and an index\n",
        "(cv) that you will later use for cross-validation. The\n",
        "text has already been tokenised and POS-tagged for you. Your algorithm\n",
        "should read in the text, **lowercase it**, store the words and their\n",
        "frequencies in an appropriate data structure that allows for easy\n",
        "computation of the probabilities used in the Naive Bayes algorithm, and\n",
        "then make predictions for new instances.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vEpyQSBSkb33"
      },
      "source": [
        "#### (Q2.1) Unseen words (1pt)\n",
        "The presence of words in the test dataset that\n",
        "have not been seen during training can cause probabilities in the Naive Bayes classifier to equal $0$.\n",
        "These can be words which are unseen in both positive and negative training reviews (case 1), but also words which are seen in reviews _of only one sentiment class_ in the training dataset (case 2). In both cases, **you should skip these words for both classes at test time**.  What would be the problem instead with skipping words only for one class in case 2?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BanFiYYnoxDW"
      },
      "source": [
        "If a word only appears in one class during training and we don't skip it during testing, then this would lead to bias. NB classifier multiplies the individual probabilities of each word in a document and so, a probability of $0$ for any word would cause the probability of the document belonging to the class that did not include the word to become zero. Hence, the bias would be towards the class that contained the word during training since it won't get affected in the multiplication of probabilities. This could result in misclassification."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gsZRhaI3WvzC"
      },
      "source": [
        "#### (Q2.2) Train your classifier on (positive and negative) reviews with cv-value 000-899, and test it on the remaining (positive and negative) reviews cv900–cv999.  Report results using classification accuracy as your evaluation metric. Your  features are the word vocabulary. The value of a feature is the count of that feature (word) in the document. (2pts)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IjcN5YvmCoQ1",
        "outputId": "8e16f2f0-0a68-4dd0-c759-58bb6d27cc72"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.82\n"
          ]
        }
      ],
      "source": [
        "# YOUR CODE HERE\n",
        "\n",
        "# computing prior P(c)\n",
        "def calc_prior_probs(doc_class_counts, total_docs):\n",
        "  priors = {}\n",
        "  for c in doc_class_counts:\n",
        "    priors[c] = math.log(doc_class_counts[c] / total_docs)\n",
        "\n",
        "  return priors\n",
        "\n",
        "# calculating conditional probs P(word|c)\n",
        "def calc_cond_probs(freqs, word_total, vocab):\n",
        "  cond_probs_POS = {}\n",
        "  cond_probs_NEG = {}\n",
        "\n",
        "  for word in vocab:\n",
        "    # overcoming issue of unseen words by\n",
        "    # skipping words that have frequency of 0 in either one of the classes\n",
        "    if freqs['POS'][word] == 0 or freqs['NEG'][word] == 0:\n",
        "      continue\n",
        "\n",
        "    if freqs['POS'][word] > 0:\n",
        "      word_freq_POS = freqs['POS'][word]\n",
        "      cond_probs_POS[word] = math.log(word_freq_POS / word_total['POS'])\n",
        "\n",
        "    if freqs['NEG'][word] > 0:\n",
        "      word_freq_NEG = freqs['NEG'][word]\n",
        "      cond_probs_NEG[word] = math.log(word_freq_NEG / word_total['NEG'])\n",
        "\n",
        "  return cond_probs_POS, cond_probs_NEG\n",
        "\n",
        "# predicting\n",
        "def classify_reviews(review, cond_probs, prior_probs):\n",
        "  results = {'POS': prior_probs['POS'], 'NEG': prior_probs['NEG']}\n",
        "\n",
        "  # for each word in a new review, adding the sum of the log probabs\n",
        "  for sentence in review['content']:\n",
        "    for token, post_tag in sentence:\n",
        "      word = token.lower()\n",
        "      # only ad  the log probs of word in it exists in both of the classes\n",
        "      if word in cond_probs['POS'] and word in cond_probs['NEG']:\n",
        "        results['POS'] += cond_probs['POS'].get(word, 0)\n",
        "        results['NEG'] += cond_probs['NEG'].get(word, 0)\n",
        "\n",
        "  # finding argmax of class\n",
        "  if results['POS'] > results['NEG']:\n",
        "    return 'POS'\n",
        "  else:\n",
        "    return 'NEG'\n",
        "\n",
        "# splitting data\n",
        "def split_data(reviews):\n",
        "  train_set = []\n",
        "  test_set = []\n",
        "  for review in reviews:\n",
        "    if review['cv'] < 900:\n",
        "      train_set.append(review)\n",
        "    else:\n",
        "      test_set.append(review)\n",
        "\n",
        "  return train_set, test_set\n",
        "\n",
        "def train(train_set):\n",
        "  # preprocessing and counting\n",
        "  vocab = set()\n",
        "  doc_class_count = {'POS': 0, 'NEG': 0}\n",
        "  word_class_count = {'POS': 0, 'NEG': 0}\n",
        "  word_freqs_class = {'POS': Counter(), 'NEG': Counter()}\n",
        "\n",
        "  for review in train_set:\n",
        "    doc_class_count[review['sentiment']] += 1\n",
        "    for sentence in review['content']:\n",
        "      for token, post_tag in sentence:\n",
        "        word = token.lower()\n",
        "        vocab.add(word)\n",
        "        word_freqs_class[review['sentiment']][word] += 1\n",
        "        word_class_count[review['sentiment']] += 1\n",
        "\n",
        "  # calculating the prior probabilities\n",
        "  total_doc_count = sum(doc_class_count.values())\n",
        "  log_prior_probs = calc_prior_probs(doc_class_count, total_doc_count)\n",
        "\n",
        "  # calculating the conditional probabilities\n",
        "  log_cond_probs_POS, log_cond_probs_NEG = calc_cond_probs(word_freqs_class, word_class_count, vocab)\n",
        "  log_cond_probs = {'POS': log_cond_probs_POS, 'NEG': log_cond_probs_NEG}\n",
        "\n",
        "  return vocab, log_prior_probs, log_cond_probs\n",
        "\n",
        "def test(test_set, vocab, log_prior_probs, log_cond_probs):\n",
        "  preds_correct_count = 0\n",
        "\n",
        "  for review in test_set:\n",
        "    predicted_class = classify_reviews(review, log_cond_probs, log_prior_probs)\n",
        "    true_class = review['sentiment']\n",
        "    if predicted_class == true_class:\n",
        "      preds_correct_count += 1\n",
        "\n",
        "  # computing accuracy\n",
        "  accuracy = preds_correct_count / len(test_set)\n",
        "  return accuracy\n",
        "\n",
        "\n",
        "train_set, test_set = split_data(reviews)\n",
        "vocab, log_prior_probs, log_cond_probs = train(train_set)\n",
        "accuracy = test(test_set, vocab, log_prior_probs, log_cond_probs)\n",
        "print(\"Accuracy: {:.2f}\".format(accuracy))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0INK-PBoM6CB"
      },
      "source": [
        "#### (Q2.3) Would you consider accuracy to also be a good way to evaluate your classifier in a situation where 90% of your data instances are of positive movie reviews? (1pt)\n",
        "\n",
        "Simulate this scenario by keeping the positive reviews\n",
        "data unchanged, but only using negative reviews cv000–cv089 for\n",
        "training, and cv900–cv909 for testing. Calculate the classification\n",
        "accuracy, and explain what changed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oFbcsYlipBAw"
      },
      "source": [
        "If 90% of the data instances are positive movie reviews, then we obtain significant class imbalance. In such case, the classifier might be biased towards the majority class (positive reviews), and so, accuracy would not be a fair performance measure since it doesn't account for the distribution of classes.\n",
        "\n",
        "In our simulated scenario below, where we reduced the number of negative reviews to match the imbalance, the classifier's accuracy reduced from 0.82 to 0.60. This may be the result of our classifier having less negative review examples to learn from, which reduces its ability to accurately identify negative sentiments. Despite the drop, an accuracy of 0.60 may still seem high. However, this is mainly because the test set in this case is heavily baised towards positive reviews. The classifier might be misclassifying many of the negative reviews.\n",
        "\n",
        "Hence, we should instead evaluate our model using other metrics/performance measures, such as but not limited to precision, recall, or the F1 score. These metrics are more balanced as they evaluate performance in terms of both classes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GWDkt5ZrrFGp",
        "outputId": "1d282c0b-3e9e-4e32-afb2-d553e778c7a7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.60\n"
          ]
        }
      ],
      "source": [
        "# splitting data - imbalanced scenario\n",
        "def split_data_imbalanced(reviews):\n",
        "  train_set = []\n",
        "  test_set = []\n",
        "  for review in reviews:\n",
        "    if review['cv'] < 900:\n",
        "      if review['sentiment'] == 'POS' or (review['sentiment'] == 'NEG' and review['cv'] < 90):\n",
        "        train_set.append(review)\n",
        "    else:\n",
        "      if review['sentiment'] == 'POS' or (review['sentiment'] == 'NEG' and review['cv'] <= 909):\n",
        "        test_set.append(review)\n",
        "\n",
        "  return train_set, test_set\n",
        "\n",
        "\n",
        "# same as above\n",
        "train_set, test_set = split_data_imbalanced(reviews)\n",
        "vocab, log_prior_probs, log_cond_probs = train(train_set)\n",
        "accuracy = test(test_set, vocab, log_prior_probs, log_cond_probs)\n",
        "print(\"Accuracy: {:.2f}\".format(accuracy))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6wJzcHX3WUDm"
      },
      "source": [
        "## Smoothing (1pt)\n",
        "\n",
        "As mentioned above, the presence of words in the test dataset that\n",
        "have not been seen during training can cause probabilities in the Naive\n",
        "Bayes classifier to be $0$, thus making that particular test instance\n",
        "undecidable. The standard way to mitigate this effect (as well as to\n",
        "give more clout to rare words) is to use smoothing, in which the\n",
        "probability fraction\n",
        "$$\\frac{\\text{count}(w_i, c)}{\\sum\\limits_{w\\in V} \\text{count}(w, c)}$$ for a word\n",
        "$w_i$ becomes\n",
        "$$\\frac{\\text{count}(w_i, c) + \\text{smoothing}(w_i)}{\\sum\\limits_{w\\in V} \\text{count}(w, c) + \\sum\\limits_{w \\in V} \\text{smoothing}(w)}$$\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PBNIcbwUWphC"
      },
      "source": [
        "#### (Q2.4) Implement Laplace feature smoothing (1pt)\n",
        "Implement Laplace smoothing, i.e., smoothing with a constant value ($smoothing(w) = \\kappa, \\forall w \\in V$), in your Naive\n",
        "Bayes classifier’s code, and report the accuracy.\n",
        "Use $\\kappa = 1$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g03yflCc9kpW",
        "outputId": "0e61f0e1-4efd-41f7-e45f-726401fa9fe5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.91\n"
          ]
        }
      ],
      "source": [
        "# YOUR CODE HERE\n",
        "\n",
        "# calculating conditional probs P(word|c) - with Laplace smoothing\n",
        "def calc_cond_probs(freqs, word_total, vocab):\n",
        "  cond_probs_POS = {}\n",
        "  cond_probs_NEG = {}\n",
        "\n",
        "  # Laplace smoothing\n",
        "  smoothing = 1\n",
        "  vocab_len = len(vocab)\n",
        "\n",
        "  for word in vocab:\n",
        "    # avoiding zero probabilities with laplace smoothing\n",
        "    word_freq_POS = freqs['POS'][word] + smoothing\n",
        "    word_freq_NEG = freqs['NEG'][word] + smoothing\n",
        "\n",
        "    cond_probs_POS[word] = math.log(word_freq_POS / (word_total['POS'] + smoothing * vocab_len))\n",
        "    cond_probs_NEG[word] = math.log(word_freq_NEG / (word_total['NEG'] + smoothing * vocab_len))\n",
        "\n",
        "  return cond_probs_POS, cond_probs_NEG\n",
        "\n",
        "# without class imbalance\n",
        "train_set, test_set = split_data(reviews)\n",
        "vocab, log_prior_probs, log_cond_probs = train(train_set)\n",
        "accuracy = test(test_set, vocab, log_prior_probs, log_cond_probs)\n",
        "print(\"Accuracy: {:.2f}\".format(accuracy))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZiGcgwba87D5"
      },
      "source": [
        "## Cross-Validation (1.5pts)\n",
        "\n",
        "A serious danger in using Machine Learning on small datasets, with many\n",
        "iterations of slightly different versions of the algorithms, is ending up with Type III errors, also called the “testing hypotheses\n",
        "suggested by the data” errors. This type of error occurs when we make\n",
        "repeated improvements to our classifiers by playing with features and\n",
        "their processing, but we don’t get a fresh, never-before seen test\n",
        "dataset every time. Thus, we risk developing a classifier that gets better\n",
        "and better on our data, but only gets worse at generalizing to new, unseen data. In other words, we risk developping a classifier that overfits.\n",
        "\n",
        "A simple method to guard against Type III errors is to use\n",
        "Cross-Validation. In **N-fold Cross-Validation**, we divide the data into N\n",
        "distinct chunks, or folds. Then, we repeat the experiment N times: each\n",
        "time holding out one of the folds for testing, training our classifier\n",
        "on the remaining N - 1 data folds, and reporting performance on the\n",
        "held-out fold. We can use different strategies for dividing the data:\n",
        "\n",
        "-   Consecutive splitting:\n",
        "  - cv000–cv099 = Split 1\n",
        "  - cv100–cv199 = Split 2\n",
        "  - etc.\n",
        "  \n",
        "-   Round-robin splitting (mod 10):\n",
        "  - cv000, cv010, cv020, … = Split 1\n",
        "  - cv001, cv011, cv021, … = Split 2\n",
        "  - etc.\n",
        "\n",
        "-   Random sampling/splitting\n",
        "  - Not used here (but you may choose to split this way in a non-educational situation)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8OeLcbSauGtR"
      },
      "source": [
        "#### (Q2.5) Write the code to implement 10-fold cross-validation using round-robin splitting for your Naive Bayes classifier from Q2.4 and compute the 10 accuracies. Report the final performance, which is the average of the performances per fold. If all splits perform equally well, this is a good sign. (1pt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q5il1Uwiy6r0",
        "outputId": "0c3aff09-a5db-46e7-a72c-96499d995b3d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "accuracy - fold 1: 0.79\n",
            "accuracy - fold 2: 0.83\n",
            "accuracy - fold 3: 0.81\n",
            "accuracy - fold 4: 0.82\n",
            "accuracy - fold 5: 0.78\n",
            "accuracy - fold 6: 0.84\n",
            "accuracy - fold 7: 0.83\n",
            "accuracy - fold 8: 0.78\n",
            "accuracy - fold 9: 0.83\n",
            "accuracy - fold 10: 0.84\n",
            "Average Accuracy: 0.82\n"
          ]
        }
      ],
      "source": [
        "# YOUR CODE\n",
        "\n",
        "# splitting data with round-robin strategy\n",
        "def split_data_round_robin(reviews, fold, k=10):\n",
        "    train_set = []\n",
        "    test_set = []\n",
        "    for review in reviews:\n",
        "        if review['cv'] % k == fold:\n",
        "            test_set.append(review)\n",
        "        else:\n",
        "            train_set.append(review)\n",
        "\n",
        "    return train_set, test_set\n",
        "\n",
        "def cross_validate(reviews, k):\n",
        "  accuracies = []\n",
        "  for fold in range(k):\n",
        "    train_set, test_set = split_data_round_robin(reviews, fold, k)\n",
        "    vocab, log_prior_probs, log_cond_probs = train(train_set)\n",
        "    accuracy = test(test_set, vocab, log_prior_probs, log_cond_probs)\n",
        "    accuracies.append(accuracy)\n",
        "\n",
        "  # averaging across all folds\n",
        "  avg_acc = sum(accuracies) / k\n",
        "\n",
        "  return accuracies, avg_acc\n",
        "\n",
        "k = 10\n",
        "all_accuracies, avg_accuracy = cross_validate(reviews, k)\n",
        "\n",
        "# observe accuracy of all splits\n",
        "f = 1\n",
        "for acc in all_accuracies:\n",
        "  print(\"accuracy - fold {}: {:.2f}\".format(f, acc))\n",
        "  f += 1\n",
        "\n",
        "print(\"Average Accuracy: {:.2f}\".format(avg_accuracy))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "otdlsDXBNyOa"
      },
      "source": [
        "#### (Q2.6) Report the variance of the 10 accuracy scores. (0.5pt)\n",
        "\n",
        "**Please report all future results using 10-fold cross-validation now\n",
        "(unless told to use the held-out test set).** Note: you're not allowed to use a library for computing the variance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZoBQm1KuNzNR",
        "outputId": "cb9129b7-6982-41d9-b4e8-effd8ca26ba9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Variance: 0.0006\n"
          ]
        }
      ],
      "source": [
        "# YOUR CODE HERE\n",
        "\n",
        "# computing variance using formula\n",
        "def calc_variance(accuracies, avg_acc, k):\n",
        "  sum_sqrd_diff = 0\n",
        "  for acc in accuracies:\n",
        "    sqrd_diff = (acc - avg_acc) ** 2\n",
        "    sum_sqrd_diff += sqrd_diff\n",
        "\n",
        "  var = sum_sqrd_diff / k\n",
        "\n",
        "  return var\n",
        "\n",
        "# updated cross validation - with variance evaluation\n",
        "def cross_validate(reviews, k):\n",
        "  accuracies = []\n",
        "  for fold in range(k):\n",
        "    train_set, test_set = split_data_round_robin(reviews, fold, k)\n",
        "    vocab, log_prior_probs, log_cond_probs = train(train_set)\n",
        "    accuracy = test(test_set, vocab, log_prior_probs, log_cond_probs)\n",
        "    accuracies.append(accuracy)\n",
        "\n",
        "  # averaging across all folds\n",
        "  avg_acc = sum(accuracies) / k\n",
        "\n",
        "  # computing variance\n",
        "  variance = calc_variance(accuracies, avg_acc, k)\n",
        "\n",
        "  return accuracies, avg_acc, variance\n",
        "\n",
        "all_accuracies, avg_accuracy, variance = cross_validate(reviews, k)\n",
        "print(\"Variance: {:.4f}\".format(variance))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s6A2zX9_BRKm"
      },
      "source": [
        "## Features, overfitting, and the curse of dimensionality\n",
        "\n",
        "In the Bag-of-Words model, ideally we would like each distinct word in\n",
        "the text to be mapped to its own dimension in the output vector\n",
        "representation. However, real world text is messy, and we need to decide\n",
        "on what we consider to be a word. For example, is “`word`\" different\n",
        "from “`Word`\", from “`word`”, or from “`words`\"? Too strict a\n",
        "definition, and the number of features explodes, while our algorithm\n",
        "fails to learn anything generalisable. Too lax, and we risk destroying\n",
        "our learning signal. In the following section, you will learn about\n",
        "confronting the feature sparsity and the overfitting problems as they\n",
        "occur in NLP classification tasks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EKK8FNt8VtcZ"
      },
      "source": [
        "### Stemming (1.5pts)\n",
        "\n",
        "To make your algorithm more robust, use stemming and hash different inflections of a word to the same feature in the BoW vector space. Please use the [Porter stemming\n",
        "    algorithm](http://www.nltk.org/howto/stem.html) from NLTK.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NxtCul1IrBi_"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE\n",
        "\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "\n",
        "# initialize globally\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "# updated classification - with stemmer\n",
        "def classify_reviews(review, cond_probs, prior_probs):\n",
        "  results = {'POS': prior_probs['POS'], 'NEG': prior_probs['NEG']}\n",
        "\n",
        "  # for each word in a new review, add the sum of the log probabs\n",
        "  for sentence in review['content']:\n",
        "    for token, post_tag in sentence:\n",
        "      # stem\n",
        "      word = stemmer.stem(token.lower())\n",
        "      # only add the log probs of word in it exists in both of the classes\n",
        "      if word in cond_probs['POS'] and word in cond_probs['NEG']:\n",
        "        results['POS'] += cond_probs['POS'].get(word, 0)\n",
        "        results['NEG'] += cond_probs['NEG'].get(word, 0)\n",
        "\n",
        "  # finding argmax of class\n",
        "  if results['POS'] > results['NEG']:\n",
        "    return 'POS'\n",
        "  else:\n",
        "    return 'NEG'\n",
        "\n",
        "# updated train - with stemmer\n",
        "def train(train_set):\n",
        "\n",
        "  # preprocessing and counting\n",
        "  vocab = set()\n",
        "  doc_class_count = {'POS': 0, 'NEG': 0}\n",
        "  word_class_count = {'POS': 0, 'NEG': 0}\n",
        "  word_freqs_class = {'POS': Counter(), 'NEG': Counter()}\n",
        "\n",
        "  for review in train_set:\n",
        "    doc_class_count[review['sentiment']] += 1\n",
        "    for sentence in review['content']:\n",
        "      for token, post_tag in sentence:\n",
        "        # stem\n",
        "        stemmed_word = stemmer.stem(token.lower())\n",
        "        vocab.add(stemmed_word)\n",
        "        word_freqs_class[review['sentiment']][stemmed_word] += 1\n",
        "        word_class_count[review['sentiment']] += 1\n",
        "\n",
        "  # calculating the prior probabilities\n",
        "  total_doc_count = sum(doc_class_count.values())\n",
        "  log_prior_probs = calc_prior_probs(doc_class_count, total_doc_count)\n",
        "\n",
        "  # calculating the conditional probabilities\n",
        "  log_cond_probs_POS, log_cond_probs_NEG = calc_cond_probs(word_freqs_class, word_class_count, vocab)\n",
        "  log_cond_probs = {'POS': log_cond_probs_POS, 'NEG': log_cond_probs_NEG}\n",
        "\n",
        "  return vocab, log_prior_probs, log_cond_probs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6SrJ1BeLXTnk"
      },
      "source": [
        "#### (Q2.7): How does the performance of your classifier change when you use stemming on your training and test datasets? (1pt)\n",
        "Use cross-validation to evaluate the classifier.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gYqKBOiIrInT",
        "outputId": "fbfe3f0d-ab6a-4ced-995a-8f531b6901fb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average Accuracy with stemming: 0.81\n",
            "Variance with stemming: 0.0007\n"
          ]
        }
      ],
      "source": [
        "# YOUR ANSWER HERE\n",
        "\n",
        "all_accuracies, avg_accuracy, variance = cross_validate(reviews, k)\n",
        "print(\"Average Accuracy with stemming: {:.2f}\".format(avg_accuracy))\n",
        "print(\"Variance with stemming: {:.4f}\".format(variance))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JkDHVq_1XUVP"
      },
      "source": [
        "#### (Q2.8) What happens to the number of features (i.e., the size of the vocabulary) when using stemming as opposed to (Q2.4)? (0.5pt)\n",
        "Give actual numbers. You can use the held-out training set to determine these."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MA3vee5-rJyy",
        "outputId": "8f50d0ce-11fe-4dca-d7d6-4e823d62749f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vocabulary size without stemming: 34662\n",
            "Vocabulary size with stemming: 24639\n",
            "With stemming, the vocabulary size is reduced since words are transformed to their root form and this groups similar words together, decreasing the total number of unique words in the vocabulary.\n"
          ]
        }
      ],
      "source": [
        "# YOUR CODE HERE\n",
        "\n",
        "# updated train - with stemmer and reutrning vocab size\n",
        "def train(train_set, stemming=False):\n",
        "\n",
        "  # preprocessing and counting\n",
        "  vocab = set()\n",
        "  doc_class_count = {'POS': 0, 'NEG': 0}\n",
        "  word_class_count = {'POS': 0, 'NEG': 0}\n",
        "  word_freqs_class = {'POS': Counter(), 'NEG': Counter()}\n",
        "\n",
        "  for review in train_set:\n",
        "    doc_class_count[review['sentiment']] += 1\n",
        "    for sentence in review['content']:\n",
        "      for token, post_tag in sentence:\n",
        "        word = token.lower()\n",
        "        if stemming:\n",
        "          # stem\n",
        "          word = stemmer.stem(word)\n",
        "        vocab.add(word)\n",
        "        word_freqs_class[review['sentiment']][word] += 1\n",
        "        word_class_count[review['sentiment']] += 1\n",
        "\n",
        "  # calculating the prior probabilities\n",
        "  total_doc_count = sum(doc_class_count.values())\n",
        "  log_prior_probs = calc_prior_probs(doc_class_count, total_doc_count)\n",
        "\n",
        "  # calculating the conditional probabilities\n",
        "  log_cond_probs_POS, log_cond_probs_NEG = calc_cond_probs(word_freqs_class, word_class_count, vocab)\n",
        "  log_cond_probs = {'POS': log_cond_probs_POS, 'NEG': log_cond_probs_NEG}\n",
        "\n",
        "  return vocab, log_prior_probs, log_cond_probs, len(vocab)\n",
        "\n",
        "\n",
        "vocab, log_prior_probs, log_cond_probs, vocab_size_without_stemming = train(train_set)\n",
        "vocab, log_prior_probs, log_cond_probs, vocab_size_with_stemming = train(train_set, stemming=True)\n",
        "\n",
        "print(f\"Vocabulary size without stemming: {vocab_size_without_stemming}\")\n",
        "print(f\"Vocabulary size with stemming: {vocab_size_with_stemming}\")\n",
        "print(f\"With stemming, the vocabulary size is reduced since words \\\n",
        "are transformed to their root form and this groups similar words together, \\\n",
        "decreasing the total number of unique words in the vocabulary.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SoazfxbNV5Lq"
      },
      "source": [
        "### N-grams (1.5pts)\n",
        "\n",
        "A simple way of retaining some of the word\n",
        "order information when using bag-of-words representations is to use **n-gram** features.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OHjy3I7-qWiu"
      },
      "source": [
        "#### (Q2.9) Retrain your classifier from (Q2.4) using **unigrams+bigrams** and **unigrams+bigrams+trigrams** as features. (1pt)\n",
        "Report accuracy and compare it with that of the approaches you have previously implemented. You are allowed to use NLTK to build n-grams from sentences."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eYuKMTOpq9jz"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE\n",
        "\n",
        "from nltk.util import bigrams, trigrams\n",
        "from collections import Counter\n",
        "\n",
        "def create_ngrams(sentences, n):\n",
        "    ngrams_list = []\n",
        "    for sentence in sentences:\n",
        "        tokens = [token.lower() for token, pos_tag in sentence]\n",
        "\n",
        "        #start and end tokens for bigrams and trigrams\n",
        "        if n > 1:\n",
        "            tokens = ['<s>'] + tokens + ['</s>']\n",
        "\n",
        "        if n == 1:\n",
        "            ngrams_list.extend(tokens)\n",
        "        elif n == 2:\n",
        "            ngrams_list.extend([' '.join(gram) for gram in bigrams(tokens)])\n",
        "        elif n == 3:\n",
        "            ngrams_list.extend([' '.join(gram) for gram in trigrams(tokens)])\n",
        "\n",
        "    return ngrams_list\n",
        "\n",
        "\n",
        "\n",
        "def train_ngram(train_set, n):\n",
        "    vocab = set()\n",
        "    doc_class_count = {'POS': 0, 'NEG': 0}\n",
        "    word_class_count = {'POS': 0, 'NEG': 0}\n",
        "    word_freqs_class = {'POS': Counter(), 'NEG': Counter()}\n",
        "\n",
        "    for review in train_set:\n",
        "        doc_class_count[review['sentiment']] += 1\n",
        "        ngrams = create_ngrams(review['content'], n)\n",
        "        for gram in ngrams:\n",
        "            vocab.add(gram)\n",
        "            word_freqs_class[review['sentiment']][gram] += 1\n",
        "            word_class_count[review['sentiment']] += 1\n",
        "\n",
        "    # calculating prior probabilities\n",
        "    total_doc_count = sum(doc_class_count.values())\n",
        "    log_prior_probs = calc_prior_probs(doc_class_count, total_doc_count)\n",
        "\n",
        "    # calculating conditional probabilities\n",
        "    log_cond_probs_POS, log_cond_probs_NEG = calc_cond_probs(word_freqs_class, word_class_count, vocab)\n",
        "    log_cond_probs = {'POS': log_cond_probs_POS, 'NEG': log_cond_probs_NEG}\n",
        "\n",
        "    return vocab, log_prior_probs, log_cond_probs, len(vocab)\n",
        "\n",
        "def test_ngram(test_set, vocab, log_prior_probs, log_cond_probs, n):\n",
        "  preds_correct_count = 0\n",
        "\n",
        "  for review in test_set:\n",
        "      # n-grams for the review\n",
        "      ngrams = create_ngrams(review['content'], n)\n",
        "\n",
        "      # probabilities with prior probabilities\n",
        "      results = {'POS': log_prior_probs['POS'], 'NEG': log_prior_probs['NEG']}\n",
        "\n",
        "      #calculatin probabilities for each class based on the n-grams\n",
        "      for gram in ngrams:\n",
        "          if gram in vocab:\n",
        "              results['POS'] += log_cond_probs['POS'].get(gram, 0)\n",
        "              results['NEG'] += log_cond_probs['NEG'].get(gram, 0)\n",
        "\n",
        "      predicted_class = 'POS' if results['POS'] > results['NEG'] else 'NEG'\n",
        "      true_class = review['sentiment']\n",
        "\n",
        "      if predicted_class == true_class:\n",
        "          preds_correct_count += 1\n",
        "\n",
        "  accuracy = preds_correct_count / len(test_set)\n",
        "  return accuracy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0OPJ-4hKxnW9",
        "outputId": "90919eab-6bd2-42b3-8f93-2e6fcd289087"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy with n-grams: 0.50\n"
          ]
        }
      ],
      "source": [
        "\n",
        "n = 3\n",
        "train_set, test_set = split_data(reviews)\n",
        "vocab, log_prior_probs, log_cond_probs,_ = train_ngram(train_set, n)\n",
        "accuracy = test(test_set, vocab, log_prior_probs, log_cond_probs)\n",
        "print(\"Accuracy with n-grams: {:.2f}\".format(accuracy))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tcbGkS0u5sBE",
        "outputId": "6c3c1000-3032-419f-d1fe-9485dbb414b8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy with n-grams: 0.50\n"
          ]
        }
      ],
      "source": [
        "\n",
        "n = 2\n",
        "train_set, test_set = split_data(reviews)\n",
        "vocab, log_prior_probs, log_cond_probs,_ = train_ngram(train_set, n)\n",
        "accuracy = test(test_set, vocab, log_prior_probs, log_cond_probs)\n",
        "print(\"Accuracy with n-grams: {:.2f}\".format(accuracy))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HDSZnDtG5fVp"
      },
      "source": [
        "The bigram model uses a combination of unigrams and bigrams as features. The drop in accuracy compared to the unigram models is maybe because the addition of bigrams may have introduced too much complexity or sparsity into the model, going into overfitting to the test set. So, these n-grams might be causing the model to overfit to the training data. The same reason for the bigram model is for the trigram model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dVrGGArkrWoL"
      },
      "source": [
        "\n",
        "#### Q2.10: How many features does the BoW model have to take into account now? (0.5pt)\n",
        "How would you expect the number of features to increase theoretically (e.g., linear, square, cubed, exponential)? How does this number compare, in practice, to the number of features at (Q2.8)?\n",
        "\n",
        "Use the held-out training set once again for this.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yEGZ9SV8pPaa"
      },
      "source": [
        "*This substantial increase presented from unigrams, bigrams, trigrams in the number of features can lead to a \"curse of dimensionality\". This results in models that overfit the training data. Unigram with stemming to unigram without stemming, this increases because stemming reduces words to their root form. Unigram without stemming to bigrams around 12-fold increase,less than the squared increase that might be theoretically expected due to linguistic constraints cause not all possible word pairs occur in natural language. Bigrams to trigrams is over double, which is also less than the theoretical cubed increase maybe cause consecutive word triplets is constrained by grammar.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_z8sAJeUrdtM",
        "outputId": "f459a423-cc59-4176-b7b3-448981d690de"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vocabulary size with unigrams: 45348\n",
            "Vocabulary size with bigrams: 424913\n",
            "Vocabulary size with trigrams: 915003\n"
          ]
        }
      ],
      "source": [
        "# YOUR CODE HERE\n",
        "\n",
        "\n",
        "\n",
        "_, _, _, vocab_size_unigrams = train_ngram(train_set, 1)\n",
        "_, _, _, vocab_size_bigrams = train_ngram(train_set, 2)\n",
        "_, _, _, vocab_size_trigrams = train_ngram(train_set, 3)\n",
        "\n",
        "print(f\"Vocabulary size with unigrams: {vocab_size_unigrams}\")\n",
        "print(f\"Vocabulary size with bigrams: {vocab_size_bigrams}\")\n",
        "print(f\"Vocabulary size with trigrams: {vocab_size_trigrams}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CHWKDL3YV6vh"
      },
      "source": [
        "# (3) Support Vector Machines (4pts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hJSYhcVaoJGt"
      },
      "source": [
        "Though simple to understand, implement, and debug, one\n",
        "major problem with the Naive Bayes classifier is that its performance\n",
        "deteriorates (becomes skewed) when it is being used with features which\n",
        "are not independent (i.e., are correlated). Another popular classifier\n",
        "that doesn’t scale as well to big data, and is not as simple to debug as\n",
        "Naive Bayes, but that doesn’t assume feature independence is the Support\n",
        "Vector Machine (SVM) classifier.\n",
        "\n",
        "You can find more details about SVMs in Chapter 7 of Bishop: Pattern Recognition and Machine Learning.\n",
        "Other sources for learning SVM:\n",
        "* http://web.mit.edu/zoya/www/SVM.pdf\n",
        "* http://www.cs.columbia.edu/~kathy/cs4701/documents/jason_svm_tutorial.pdf\n",
        "* https://pythonprogramming.net/support-vector-machine-intro-machine-learning-tutorial/\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Use the scikit-learn implementation of\n",
        "[SVM](http://scikit-learn.org/stable/modules/svm.html) with the default parameters. (You are not expected to perform any hyperparameter tuning, but feel free to do it if you think it gives you good insights for the discussion in question 5.)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0LnzNtQBV8gr"
      },
      "source": [
        "#### (Q3.1): Train SVM and compare to Naive Bayes (2pts)\n",
        "\n",
        "Train an SVM classifier (sklearn.svm.LinearSVC) using the features collected for Naive Bayes. Compare the\n",
        "classification performance of the SVM classifier to that of the Naive\n",
        "Bayes classifier with smoothing.\n",
        "Use cross-validation to evaluate the performance of the classifiers.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JBscui8Mvoz0",
        "outputId": "2ba7f9e7-6fda-41da-fef9-6b688502f8ee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Naive Bayes Average Accuracy: 0.8225000000000001\n",
            "SVM Average Accuracy: 0.8655000000000002\n"
          ]
        }
      ],
      "source": [
        "# YOUR CODE HERE\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "texts = [\" \".join([\" \".join(token for token, pos_tag in sentence) for sentence in review['content']]) for review in reviews]\n",
        "labels = [review['sentiment'] for review in reviews]\n",
        "\n",
        "# TF-IDF Vectorizer\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "\n",
        "# Naive Bayes Classifier Pipeline\n",
        "nb_pipeline = make_pipeline(tfidf_vectorizer, MultinomialNB())\n",
        "\n",
        "# SVM Classifier Pipeline\n",
        "svm_pipeline = make_pipeline(tfidf_vectorizer, LinearSVC())\n",
        "\n",
        "# cross-validation\n",
        "nb_scores = cross_val_score(nb_pipeline, texts, labels, cv=5)\n",
        "svm_scores = cross_val_score(svm_pipeline, texts, labels, cv=5)\n",
        "\n",
        "print(f\"Naive Bayes Average Accuracy: {nb_scores.mean()}\")\n",
        "print(f\"SVM Average Accuracy: {svm_scores.mean()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4PZ-3nGqCa9O"
      },
      "source": [
        "The improvement could be due to SVM's ability to handle the dependency between features better than Naive Bayes. SVM finds the optimal hyperplane that maximizes the margin between classes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ifXVWcK0V9qY"
      },
      "source": [
        "### POS disambiguation (2pts)\n",
        "\n",
        "Now add in part-of-speech features. You will find the\n",
        "movie review dataset has already been POS-tagged for you ([here](https://catalog.ldc.upenn.edu/docs/LDC99T42/tagguid1.pdf) you find the tagset). Try to\n",
        "replicate the results obtained by Pang et al. (2002).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xA3I82o4oWGu"
      },
      "source": [
        "####(Q3.2) Replace your features with word+POS features, and report performance with the SVM. Use cross-validation to evaluate the classifier and compare the results with (Q3.1). Does part-of-speech information help? Explain why this may be the case. (1pt)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NOvjYe-t2Br6",
        "outputId": "ced4f465-df4e-4878-df3e-1397f441c3c2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SVM Average Accuracy with POS features: 0.8615\n"
          ]
        }
      ],
      "source": [
        "# YOUR CODE HERE\n",
        "\n",
        "texts_with_pos = [\" \".join([f\"{token}_{pos_tag}\" for sentence in review['content'] for token, pos_tag in sentence]) for review in reviews]\n",
        "\n",
        "\n",
        "# SVM Classifier Pipeline with TF-IDF Vectorizer\n",
        "svm_pipeline_with_pos = make_pipeline(TfidfVectorizer(), LinearSVC())\n",
        "\n",
        "# cross-validation\n",
        "svm_scores_with_pos = cross_val_score(svm_pipeline_with_pos, texts_with_pos, labels, cv=5)\n",
        "\n",
        "print(f\"SVM Average Accuracy with POS features: {svm_scores_with_pos.mean()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L0dt_oQupUNe"
      },
      "source": [
        "*Accuracy slightly decreased when POS features were used compared to using just the words. In movie reviews, maybe the sentiment might be more related to specific words and their modifiers like adjectives or adverbs rather than the syntactic structure of the sentences.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Su-3w87eMW0w"
      },
      "source": [
        "#### (Q3.3) Discard all closed-class words from your data (keep only nouns, verbs, adjectives, and adverbs), and report performance. Does this help? Use cross-validation to evaluate the classifier and compare the results with (Q3.2). Are closed-class words detrimental to the classifier? Explain why this may be the case. (1pt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CCUPlPozCYUX",
        "outputId": "98f1fa70-60a5-40e7-ba72-ed5ccd2a2e86"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SVM Average Accuracy with filtered POS (open-class words only): 0.8560000000000001\n"
          ]
        }
      ],
      "source": [
        "# YOUR CODE HERE\n",
        "\n",
        "# keep only nouns, verbs, adjectives, and adverbs\n",
        "def is_open_class(tag):\n",
        "    return tag.startswith('NN') or tag.startswith('VB') or tag.startswith('JJ') or tag.startswith('RB')\n",
        "\n",
        "#texts list with filtered POS tags\n",
        "texts_filtered = [\" \".join([f\"{token}_{pos_tag}\" for sentence in review['content'] for token, pos_tag in sentence if is_open_class(pos_tag)]) for review in reviews]\n",
        "\n",
        "# SVM Classifier Pipeline with TF-IDF Vectorizer\n",
        "svm_pipeline_filtered = make_pipeline(TfidfVectorizer(), LinearSVC())\n",
        "\n",
        "#cross-validation\n",
        "svm_scores_filtered = cross_val_score(svm_pipeline_filtered, texts_filtered, labels, cv=5)\n",
        "\n",
        "print(f\"SVM Average Accuracy with filtered POS (open-class words only): {svm_scores_filtered.mean()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YaxCVrs8pWSp"
      },
      "source": [
        "**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nfwqOciAl2No"
      },
      "source": [
        "# (4) Discussion (max. 500 words). (5pts)\n",
        "\n",
        "> Based on your experiments, what are the effective features and techniques in sentiment analysis? What information do different features encode?\n",
        "Why is this important? What are the limitations of these features and techniques?\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZYuse5WLmekZ"
      },
      "source": [
        "*Based on the experiments conducted, sentiment is captured through different features and techniques, each with varying levels of effectiveness. The simplest and frequently most useful are unigrams. They pick out specific, sentiment-filled words that are powerful markers of either a positive or negative attitude. But they lack context.\n",
        "N-grams: By capturing word sequences, bigrams and trigrams are able to capture more context than unigrams. Useful to recognising phrases with a sentiment distinct from the words used alone. They do, however, expand the feature space, which can result in overfitting and sparsity. Part-of-Speech (POS) Tags: Using POS tags makes it easier to distinguish between the functions of words in sentences. Adverbs and adjectives, for instance, frequently convey strong sentiment signals. However, the results of the experiment suggested that POS tagging information might not greatly enhance performance, perhaps as a result of the complexity that is added. Open-Class Words: Theoretically, concentrating on nouns, verbs, adjectives, and adverbs aids in focusing on content words that have semantic meaning. Removing closed-class terms can remove important structural information. Stemming: By combining various word nuances, reducing words to their basic form can aid in the generalisation of sentiment analysis. However, it might also lose the nuanced sentiment variations that different word forms convey. In terms of limistations, Unigrams and N-grams: These features are sensitive to the dimensionality and sparsity of the feature space. POS Tags: is not good if the syntactic structure is less relevant to the sentiment. Stemming: under- or over-stemming can lead to ineffective generalisation. Open-Class Words: Eliminating closed-class words could leave out linguistic structural components that influence sentiment. Limitations come probably as the inability to capture sarcasm, idiomatic expressions, or context beyond local word patterns, so probably the need for more advanced models, incorporating syntactic parsing, semantic analysis, or deep learning, to understand the whole spectrum of sentiment expression.*\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iwaKwfWQhRk_"
      },
      "source": [
        "# Submission\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aOUeaET5ijk-"
      },
      "outputs": [],
      "source": [
        "# Write your names and student numbers here:\n",
        "# Ekin Fergan #12652954\n",
        "# Evelyn Pomasqui #12481440"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3A9K-H6Tii3X"
      },
      "source": [
        "**That's it!**\n",
        "\n",
        "- Check if you answered all questions fully and correctly.\n",
        "- Download your completed notebook using `File -> Download .ipynb`\n",
        "- Check if your answers are all included in the file you submit.\n",
        "- Submit your .ipynb file via *Canvas*. One submission per group."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YHslatYAKBrF"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}